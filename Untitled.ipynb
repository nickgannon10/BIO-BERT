{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9df4b56-d06d-4156-a44c-85774dac8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "570d9b23-2d95-453f-a054-7d5c373eb220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import protein_tokenizer\n",
    "importlib.reload(protein_tokenizer)\n",
    "from protein_tokenizer import ProteinTokenizer\n",
    "\n",
    "tokenizer = ProteinTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0b786bdc-c43e-45ad-9a7b-09aea903917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G']\n",
      "Token IDs: [10, 8, 17, 9, 7, 4, 0, 1, 5]\n",
      "Recovered Tokens: ['M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G']\n",
      "Recovered Sequence: MKVLIFACG\n"
     ]
    }
   ],
   "source": [
    "def test_basic_tokenization():\n",
    "    tokenizer = ProteinTokenizer()\n",
    "    sequence = \"MKVLIFACG\"\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"Token IDs: {ids}\")\n",
    "    recovered_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"Recovered Tokens: {recovered_tokens}\")\n",
    "    recovered_sequence = tokenizer.convert_tokens_to_string(recovered_tokens)\n",
    "    print(f\"Recovered Sequence: {recovered_sequence}\")\n",
    "\n",
    "test_basic_tokenization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b0f88a5-f3e5-4eb6-a3c7-56c7d474ed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with Special Tokens: ['[CLS]', 'M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G', '[SEP]']\n",
      "Token IDs with Special Tokens: [23, 10, 8, 17, 9, 7, 4, 0, 1, 5, 24]\n",
      "Recovered Tokens with Special Tokens: ['[CLS]', 'M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G', '[SEP]']\n",
      "Recovered Sequence with Special Tokens: [CLS]MKVLIFACG[SEP]\n"
     ]
    }
   ],
   "source": [
    "def test_special_token_integration():\n",
    "    tokenizer = ProteinTokenizer()\n",
    "    sequence = \"[CLS]MKVLIFACG[SEP]\"\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    print(f\"Tokens with Special Tokens: {tokens}\")\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"Token IDs with Special Tokens: {ids}\")\n",
    "    recovered_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"Recovered Tokens with Special Tokens: {recovered_tokens}\")\n",
    "    recovered_sequence = tokenizer.convert_tokens_to_string(recovered_tokens)\n",
    "    print(f\"Recovered Sequence with Special Tokens: {recovered_sequence}\")\n",
    "\n",
    "test_special_token_integration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed4fa40a-a261-4516-b930-cc07b23ce36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with Unknown Characters: ['M', 'K', 'V', 'L', '[UNK]', 'I', 'F', 'A', 'C', 'G']\n",
      "Token IDs with Unknown Characters: [10, 8, 17, 9, 22, 7, 4, 0, 1, 5]\n",
      "Recovered Tokens with Unknown Characters: ['M', 'K', 'V', 'L', '[UNK]', 'I', 'F', 'A', 'C', 'G']\n",
      "Recovered Sequence with Unknown Characters: MKVL[UNK]IFACG\n"
     ]
    }
   ],
   "source": [
    "def test_unknown_characters():\n",
    "    tokenizer = ProteinTokenizer()\n",
    "    sequence = \"MKVLZIFACG\"  # Z is not a valid amino acid\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    print(f\"Tokens with Unknown Characters: {tokens}\")\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"Token IDs with Unknown Characters: {ids}\")\n",
    "    recovered_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"Recovered Tokens with Unknown Characters: {recovered_tokens}\")\n",
    "    recovered_sequence = tokenizer.convert_tokens_to_string(recovered_tokens)\n",
    "    print(f\"Recovered Sequence with Unknown Characters: {recovered_sequence}\")\n",
    "\n",
    "test_unknown_characters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36d61200-9cc6-4979-88b9-09ac2d194a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with Full Workflow: ['[CLS]', 'M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G', '[SEP]']\n",
      "Token IDs with Full Workflow: [23, 10, 8, 17, 9, 7, 4, 0, 1, 5, 24]\n",
      "Recovered Tokens with Full Workflow: ['[CLS]', 'M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G', '[SEP]']\n",
      "Recovered Sequence with Full Workflow: [CLS]MKVLIFACG[SEP]\n"
     ]
    }
   ],
   "source": [
    "def test_full_special_token_workflow():\n",
    "    tokenizer = ProteinTokenizer()\n",
    "    sequence = tokenizer.cls_token + \"MKVLIFACG\" + tokenizer.sep_token\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    print(f\"Tokens with Full Workflow: {tokens}\")\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(f\"Token IDs with Full Workflow: {ids}\")\n",
    "    recovered_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"Recovered Tokens with Full Workflow: {recovered_tokens}\")\n",
    "    recovered_sequence = tokenizer.convert_tokens_to_string(recovered_tokens)\n",
    "    print(f\"Recovered Sequence with Full Workflow: {recovered_sequence}\")\n",
    "\n",
    "test_full_special_token_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ebb93e3b-8f93-42b5-8d88-16018660e995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Tokens: ['[CLS]', 'M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Token IDs: [23, 10, 8, 17, 9, 7, 4, 0, 1, 5, 24, 20, 20, 20, 20]\n",
      "Recovered Tokens: ['[CLS]', 'M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Recovered Sequence: CLS]MKVLIFACG[SE\n"
     ]
    }
   ],
   "source": [
    "def test_simple_sequence_with_special_tokens():\n",
    "    tokenizer = ProteinTokenizer()\n",
    "    sequence = tokenizer.cls_token + \"MKVLIFACG\" + tokenizer.sep_token\n",
    "    max_length = 15\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    padded_tokens = tokens + [tokenizer.pad_token] * (max_length - len(tokens))\n",
    "    print(f\"Padded Tokens: {padded_tokens}\")\n",
    "    ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    print(f\"Token IDs: {ids}\")\n",
    "    recovered_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"Recovered Tokens: {recovered_tokens}\")\n",
    "    recovered_sequence = tokenizer.convert_tokens_to_string(recovered_tokens).strip(tokenizer.pad_token)\n",
    "    print(f\"Recovered Sequence: {recovered_sequence}\")\n",
    "    \n",
    "test_simple_sequence_with_special_tokens()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e839c24c-5bd8-462d-9c61-72a158ea7a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Tokens: ['[CLS]', 'M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G', '[SEP]', 'M', 'A', 'D', 'Y', 'L', 'N', 'S', 'D', 'Y', 'R', '[SEP]']\n",
      "Token IDs: [23, 10, 8, 17, 9, 7, 4, 0, 1, 5, 24, 10, 0, 2, 19, 9, 11, 15, 2, 19, 14, 24]\n",
      "Recovered Tokens: ['[CLS]', 'M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G', '[SEP]', 'M', 'A', 'D', 'Y', 'L', 'N', 'S', 'D', 'Y', 'R', '[SEP]']\n",
      "Recovered Sequence: CLS]MKVLIFACG[SEP]MADYLNSDYR[SE\n"
     ]
    }
   ],
   "source": [
    "def test_sequence_pair_with_special_tokens():\n",
    "    tokenizer = ProteinTokenizer()\n",
    "    sequence1 = \"MKVLIFACG\"\n",
    "    sequence2 = \"MADYLNSDYR\"\n",
    "    combined_sequence = tokenizer.cls_token + sequence1 + tokenizer.sep_token + sequence2 + tokenizer.sep_token\n",
    "    max_length = 20\n",
    "    tokens = tokenizer.tokenize(combined_sequence)\n",
    "    padded_tokens = tokens + [tokenizer.pad_token] * (max_length - len(tokens))\n",
    "    print(f\"Padded Tokens: {padded_tokens}\")\n",
    "    ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    print(f\"Token IDs: {ids}\")\n",
    "    recovered_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"Recovered Tokens: {recovered_tokens}\")\n",
    "    recovered_sequence = tokenizer.convert_tokens_to_string(recovered_tokens).strip(tokenizer.pad_token)\n",
    "    print(f\"Recovered Sequence: {recovered_sequence}\")\n",
    "\n",
    "test_sequence_pair_with_special_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a02133c-75b2-4d90-afe1-03fb0725bbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated Tokens: ['[CLS]', 'M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G', '[SEP]']\n",
      "Token IDs: [23, 10, 8, 17, 9, 7, 4, 0, 1, 5, 24]\n",
      "Recovered Tokens: ['[CLS]', 'M', 'K', 'V', 'L', 'I', 'F', 'A', 'C', 'G', '[SEP]']\n",
      "Recovered Sequence: [CLS]MKVLIFACG[SEP]\n"
     ]
    }
   ],
   "source": [
    "def test_variable_length_sequences():\n",
    "    tokenizer = ProteinTokenizer()\n",
    "    sequence = tokenizer.cls_token + \"MKVLIFACG\" + tokenizer.sep_token\n",
    "    max_length = 12  # Context window smaller than sequence\n",
    "    truncated_tokens = tokenizer.tokenize(sequence)[:max_length]\n",
    "    print(f\"Truncated Tokens: {truncated_tokens}\")\n",
    "    ids = tokenizer.convert_tokens_to_ids(truncated_tokens)\n",
    "    print(f\"Token IDs: {ids}\")\n",
    "    recovered_tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"Recovered Tokens: {recovered_tokens}\")\n",
    "    recovered_sequence = tokenizer.convert_tokens_to_string(recovered_tokens)\n",
    "    print(f\"Recovered Sequence: {recovered_sequence}\")\n",
    "\n",
    "test_variable_length_sequences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e0db6-5a96-4ea2-b416-9f05952d731a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (proteinBERT.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
